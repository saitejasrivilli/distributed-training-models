model:
  name: "small"
  use_flash_attention: false
  use_gradient_checkpointing: false

training:
  batch_size: 512
  micro_batch_size: 8
  gradient_accumulation_steps: 64
  
  learning_rate: 3.0e-4
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  
  warmup_steps: 2000
  max_steps: 100000
  
  mixed_precision: "bf16"
  
  save_every: 5000
  eval_every: 1000
  log_every: 100

data:
  dataset_name: "wikitext"
  dataset_config: "wikitext-103-raw-v1"  # FIXED: Using larger dataset
  tokenizer: "gpt2"
  max_seq_len: 2048
  num_workers: 4

distributed:
  strategy: "ddp"
  backend: "nccl"

logging:
  project: "distributed-llm-training"
  run_name: "gpt-small-117M"
  log_with: "none"

compute:
  seed: 42
